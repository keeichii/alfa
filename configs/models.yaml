# Model configurations for embeddings, reranking, and tokenization

embeddings:
  model_name: "intfloat/multilingual-e5-large"  # upgraded from base for better embeddings
  # alternatives: 
  # - "intfloat/multilingual-e5-base" (faster, less accurate)
  # - "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
  device: "auto"          # must resolve to CUDA; raises if kernels absent
  batch_size: 48          # reduced batch size for large model to fit 8GB GPUs
  normalize_embeddings: true
  use_fp16: true          # enable half precision on GPU

reranker:
  model_name: "BAAI/bge-reranker-base"  # multilingual reranker (upgraded from ms-marco)
  # alternatives:
  # - "cross-encoder/ms-marco-MiniLM-L-12-v2" (English only, faster)
  # - "cross-encoder/ms-marco-MiniLM-L-6-v2" (faster, less accurate)
  device: "auto"
  batch_size: 64          # optimized batch size for GPU
  max_length: 512         # max sequence length for reranker (must match model's max_length)
  use_fp16: true
  
# Ensemble rerankers (optional, for advanced accuracy)
reranker_ensemble:
  enabled: false  # set to true to use ensemble of multiple rerankers
  models:
    - "BAAI/bge-reranker-base"
    - "cross-encoder/ms-marco-MiniLM-L-12-v2"
  weights: [0.7, 0.3]  # weights for ensemble (sum should be 1.0)
  second_pass: false  # enable second pass reranking on top candidates
  second_pass_topk: 20  # number of candidates for second pass

tokenizer:
  name: "o200k_base"
  # alternatives:
  # - "word" (simple word-based)
  # - path to huggingface tokenizer

faiss:
  index_type: "IVF256,Flat"  # "Flat", "IVF{nlist},Flat", "IVF{nlist},PQ{m}"
  # Format: "IVF{nlist},Flat" where nlist is number of clusters (e.g., 256, 512, 1024)
  # For large datasets, use larger nlist (e.g., IVF1024,Flat)
  # For very large datasets, consider PQ quantization: "IVF1024,PQ64"
  nlist: 256  # number of clusters for IVF indices (used if index_type is "IVFFlat" shorthand)
  nprobe: 10  # number of clusters to search in IVF indices (runtime parameter)
  use_gpu: true           # FAISS index is cloned to GPU
  gpu_device: 0
  use_float16: true

bm25:
  backend: "bm25s"  # "bm25s" or "pyserini"
  k1: 1.5
  b: 0.75

